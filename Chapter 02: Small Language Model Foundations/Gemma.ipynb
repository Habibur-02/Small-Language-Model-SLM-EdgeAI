{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f0bc9649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: transformers in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (4.57.6)\n",
      "Requirement already satisfied: accelerate in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: bitsandbytes in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (0.49.1)\n",
      "Requirement already satisfied: pillow in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (11.0.0)\n",
      "Requirement already satisfied: scipy in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (1.17.0)\n",
      "Requirement already satisfied: filelock in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (0.36.0)\n",
      "Requirement already satisfied: numpy>=1.17 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (2.3.2)\n",
      "Requirement already satisfied: packaging>=20.0 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (6.0.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (2026.1.15)\n",
      "Requirement already satisfied: requests in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (2.32.5)\n",
      "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.6.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
      "Requirement already satisfied: psutil in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from accelerate) (2.6.0+cu124)\n",
      "Requirement already satisfied: networkx in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.3)\n",
      "Requirement already satisfied: jinja2 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.4)\n",
      "Requirement already satisfied: sympy==1.13.1 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from requests->transformers) (3.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from requests->transformers) (3.11)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from requests->transformers) (2.5.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in e:\\smartsearchsystem\\smartsearchsystem\\lib\\site-packages (from requests->transformers) (2026.1.4)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 25.2 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate bitsandbytes pillow scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e498eccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from huggingface_hub import login\n",
    "\n",
    "# ‡¶Ü‡¶™‡¶®‡¶æ‡¶∞ ‡¶ï‡¶™‡¶ø ‡¶ï‡¶∞‡¶æ ‡¶ü‡ßã‡¶ï‡ßá‡¶®‡¶ü‡¶ø ‡¶°‡¶æ‡¶¨‡¶≤ ‡¶ï‡ßã‡¶ü‡ßá‡¶∞ ‡¶≠‡ßá‡¶§‡¶∞‡ßá ‡¶™‡ßá‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡ßÅ‡¶®\n",
    "# ‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£: login(token=\"hf_ABC123...\")\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17daa49e",
   "metadata": {},
   "source": [
    "### üöÄ ‡¶ß‡¶æ‡¶™ ‡ßß: Gemma-2-9B ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶°\n",
    "‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ Google-‡¶è‡¶∞ ‡¶∂‡¶ï‡ßç‡¶§‡¶ø‡¶∂‡¶æ‡¶≤‡ßÄ **Gemma-2-9B-Instruct** ‡¶Æ‡¶°‡ßá‡¶≤‡¶ü‡¶ø ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§\n",
    "- **`torch.bfloat16`**: ‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø ‡¶Ö‡¶™‡ßç‡¶ü‡¶ø‡¶Æ‡¶æ‡¶á‡¶ú‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶á ‡¶°‡ßá‡¶ü‡¶æ ‡¶ü‡¶æ‡¶á‡¶™ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§\n",
    "- **`device_map=\"auto\"`**: ‡¶è‡¶ü‡¶ø ‡¶Ö‡¶ü‡ßã‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø ‡¶ú‡¶ø‡¶™‡¶ø‡¶â ‡¶è‡¶¨‡¶Ç ‡¶∏‡¶ø‡¶™‡¶ø‡¶â-‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶¨‡ßç‡¶Ø‡¶æ‡¶≤‡ßá‡¶®‡ßç‡¶∏ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡ßá‡•§\n",
    "- **Hugging Face Login**: Gemma ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø `huggingface_hub.login()` ‡¶¨‡¶æ ‡¶ü‡ßã‡¶ï‡ßá‡¶® ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ ‡¶¨‡¶æ‡¶ß‡ßç‡¶Ø‡¶§‡¶æ‡¶Æ‡ßÇ‡¶≤‡¶ï‡•§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "239e51c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-2-9b-it...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aab4c901731241c39a89addae45f5e9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SmartSearchSystem\\SmartSearchSystem\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RUET\\.cache\\huggingface\\hub\\models--google--gemma-2-9b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a175c9ffefe47a2af2dbb7cc399b621",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9787491a202485f82a6d2ebb00d5fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1963c6a514b34c4e9cbc95b9c412e338",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57d6b035ed9a4abe8bfbea22d04da4d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/857 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7658c4ceb03d4b0586a04e574b59c521",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/39.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "716071a66cc6400abe02a8a1e78ced7b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 4 files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a44283128c77409cb734fef59836883e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.90G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60734eeb7e464adcaea678e6191e51ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72eacf7e172d4a0ea40fe370dd9ab7a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "856b0bcff6594411abc551320ecf3ec8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/3.67G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8df35364164940b99883e170f8a9fd89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "191521a03329447eb8fed51ae179f5dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/173 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Loaded Successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ‡¶Ø‡¶¶‡¶ø gemma-3 ‡¶ï‡¶æ‡¶ú ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá, ‡¶§‡¶¨‡ßá gemma-2-9b-it ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶® (‡¶è‡¶ü‡¶ø ‡¶®‡¶ø‡¶∂‡ßç‡¶ö‡¶ø‡¶§ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶¨‡ßá)\n",
    "# model_id = \"google/gemma-3-8b-it\"  <-- ‡¶è‡¶ü‡¶æ‡¶§‡ßá ‡¶è‡¶∞‡¶∞ ‡¶Ü‡¶∏‡¶≤‡ßá ‡¶®‡¶ø‡¶ö‡ßá‡¶∞‡¶ü‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßÅ‡¶®\n",
    "model_id = \"google/gemma-2-9b-it\"\n",
    "\n",
    "print(f\"Loading {model_id}...\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    torch_dtype=torch.bfloat16, # RTX A4500 ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶™‡¶æ‡¶∞‡¶´‡ßá‡¶ï‡ßç‡¶ü\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "print(\"Model Loaded Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae3a25fe",
   "metadata": {},
   "source": [
    "### üëÅÔ∏è ‡¶ß‡¶æ‡¶™ ‡ß®: ‡¶õ‡¶¨‡¶ø ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶è‡¶¨‡¶Ç ‡¶¨‡ßã‡¶ù‡¶æ (PaliGemma)\n",
    "Gemma ‡¶´‡ßç‡¶Ø‡¶æ‡¶Æ‡¶ø‡¶≤‡¶ø‡¶∞ ‡¶≠‡¶ø‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶π‡¶≤‡ßã **PaliGemma**‡•§ ‡¶è‡¶ü‡¶ø ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡ßá‡¶∞ ‡¶™‡¶æ‡¶∂‡¶æ‡¶™‡¶æ‡¶∂‡¶ø ‡¶õ‡¶¨‡¶ø‡¶ì ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶®‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§\n",
    "- **`AutoProcessor`**: ‡¶è‡¶ü‡¶ø ‡¶õ‡¶¨‡¶ø ‡¶è‡¶¨‡¶Ç ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü‡¶ï‡ßá ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßá ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶™‡¶æ‡¶†‡¶æ‡ßü‡•§\n",
    "- **‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞**: ‡¶Æ‡ßá‡¶°‡¶ø‡¶ï‡ßá‡¶≤ ‡¶∞‡¶ø‡¶™‡ßã‡¶∞‡ßç‡¶ü ‡¶¨‡¶ø‡¶∂‡ßç‡¶≤‡ßá‡¶∑‡¶£, ‡¶ö‡¶æ‡¶∞‡ßç‡¶ü ‡¶∞‡¶ø‡¶°‡¶ø‡¶Ç ‡¶¨‡¶æ ‡¶∏‡¶ø‡¶ï‡¶ø‡¶â‡¶∞‡¶ø‡¶ü‡¶ø ‡¶ï‡ßç‡¶Ø‡¶æ‡¶Æ‡ßá‡¶∞‡¶æ‡¶∞ ‡¶´‡ßÅ‡¶ü‡ßá‡¶ú ‡¶¨‡ßã‡¶ù‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ü‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "382bb9bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Vision Model: google/paligemma-3b-mix-224...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90282f219f3141b1ba3606cefe16a2b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/699 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SmartSearchSystem\\SmartSearchSystem\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RUET\\.cache\\huggingface\\hub\\models--google--paligemma-3b-mix-224. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a3c0ef3a25244f8b33a444bd7dd4ba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/40.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6f8a259f8f204c6e8f4efb72e961d7e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.26M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "459f047afe3b4996b1f880584a77a860",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a66b8cb2695484eab0463b44166feb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/24.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a279bbb189c04e6595f89b0dca503941",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/607 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SmartSearchSystem\\SmartSearchSystem\\Lib\\site-packages\\transformers\\models\\auto\\modeling_auto.py:2284: FutureWarning: The class `AutoModelForVision2Seq` is deprecated and will be removed in v5.0. Please use `AutoModelForImageTextToText` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "94017ad645f149c4b528060e782ed447",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.03k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5037e535ccb4e8d8d2c7922a28eb183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/62.6k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31886e5eb9a9437b92a2f1c9d2f00fd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29a0adf95c044fc2afc87b72954e21c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d37013a89b040b5a5d1c2e114745131",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80a0ef1e999a4e99ad4391afc412cd90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/1.74G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ac04167d9fb4ac697e316e863d40585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb75698479e54e969c96d19aa8283208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "You are passing both `text` and `images` to `PaliGemmaProcessor`. The processor expects special image tokens in the text, as many tokens as there are images per each text. It is recommended to add `<image>` tokens in the very beginning of your text. For this call, we will infer how many images each text has and add special tokens.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing Image...\n",
      "------------------------------\n",
      "Image Analysis Result: In this image we can see a car on the road. In the background there is a wall, door, trees and sky.\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForVision2Seq\n",
    "from PIL import Image\n",
    "import requests\n",
    "import torch\n",
    "\n",
    "# ‡ßß. ‡¶≠‡¶ø‡¶∂‡¶® ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶° (PaliGemma - Google's Vision Gemma)\n",
    "# ‡¶è‡¶ü‡¶ø ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡ß© ‡¶¨‡¶ø‡¶≤‡¶ø‡¶Ø‡¶º‡¶® ‡¶™‡ßç‡¶Ø‡¶æ‡¶∞‡¶æ‡¶Æ‡¶ø‡¶ü‡¶æ‡¶∞‡ßá‡¶∞ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶õ‡¶¨‡¶ø ‡¶¨‡ßÅ‡¶ù‡¶§‡ßá ‡¶ñ‡ßÅ‡¶¨ ‡¶¶‡¶ï‡ßç‡¶∑\n",
    "vision_model_id = \"google/paligemma-3b-mix-224\"\n",
    "\n",
    "print(f\"Loading Vision Model: {vision_model_id}...\")\n",
    "processor = AutoProcessor.from_pretrained(vision_model_id)\n",
    "vision_model = AutoModelForVision2Seq.from_pretrained(\n",
    "    vision_model_id,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    ").eval()\n",
    "\n",
    "# ‡ß®. ‡¶õ‡¶¨‡¶ø ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ (‡¶á‡¶®‡ßç‡¶ü‡¶æ‡¶∞‡¶®‡ßá‡¶ü ‡¶•‡ßá‡¶ï‡ßá ‡¶¨‡¶æ ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶´‡¶æ‡¶á‡¶≤ ‡¶•‡ßá‡¶ï‡ßá)\n",
    "# ‡¶ü‡ßá‡¶∏‡ßç‡¶ü‡¶ø‡¶Ç‡ßü‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶Ö‡¶®‡¶≤‡¶æ‡¶á‡¶® ‡¶õ‡¶¨‡¶ø ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø\n",
    "url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/car.jpg?download=true\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "# ‡¶§‡ßÅ‡¶Æ‡¶ø ‡¶ö‡¶æ‡¶á‡¶≤‡ßá ‡¶≤‡ßã‡¶ï‡¶æ‡¶≤ ‡¶õ‡¶¨‡¶ø‡¶ì ‡¶¶‡¶ø‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã: image = Image.open(\"my_image.jpg\")\n",
    "\n",
    "# ‡ß©. ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶® ‡¶§‡ßà‡¶∞‡¶ø\n",
    "prompt = \"caption en\" # PaliGemma ‡¶ï‡ßá ‡¶¨‡¶≤‡¶§‡ßá ‡¶π‡ßü ‡¶ï‡ßÄ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá (‡¶Ø‡ßá‡¶Æ‡¶®: caption, detect, vqa)\n",
    "model_inputs = processor(text=prompt, images=image, return_tensors=\"pt\").to(vision_model.device)\n",
    "input_len = model_inputs[\"input_ids\"].shape[-1]\n",
    "\n",
    "# ‡ß™. ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®\n",
    "print(\"Analyzing Image...\")\n",
    "with torch.no_grad():\n",
    "    generation = vision_model.generate(**model_inputs, max_new_tokens=100, do_sample=False)\n",
    "    generation = generation[0][input_len:]\n",
    "    decoded = processor.decode(generation, skip_special_tokens=True)\n",
    "\n",
    "print(\"-\" * 30)\n",
    "print(f\"Image Analysis Result: {decoded}\")\n",
    "print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06b3b0f6",
   "metadata": {},
   "source": [
    "### üõ†Ô∏è ‡¶ß‡¶æ‡¶™ ‡ß©: ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶ï‡¶≤‡¶ø‡¶Ç (JSON Output)\n",
    "‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá ‡¶¶‡¶ø‡ßü‡ßá ‡¶ï‡ßã‡¶° ‡¶¨‡¶æ API ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶è‡¶ü‡¶ø ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡•§\n",
    "- ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá ‡¶∂‡¶ø‡¶ñ‡¶ø‡ßü‡ßá‡¶õ‡¶ø ‡¶ï‡¶ø‡¶≠‡¶æ‡¶¨‡ßá ‡¶∏‡¶æ‡¶ß‡¶æ‡¶∞‡¶£ ‡¶â‡¶§‡ßç‡¶§‡¶∞‡ßá‡¶∞ ‡¶¨‡¶¶‡¶≤‡ßá **JSON** ‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶ü‡ßá ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡¶¶‡¶ø‡¶§‡ßá ‡¶π‡ßü‡•§\n",
    "- ‡¶è‡¶á JSON ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡ßã‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡ßá‡¶ü‡¶ø‡¶ï‡ßç‡¶Ø‡¶æ‡¶≤‡¶ø ‡¶ì‡ßü‡ßá‡¶¶‡¶æ‡¶∞ API ‡¶¨‡¶æ ‡¶°‡¶æ‡¶ü‡¶æ‡¶¨‡ßá‡¶∏ ‡¶ï‡ßÅ‡ßü‡ßá‡¶∞‡¶ø ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø‡•§ ‡¶è‡¶ü‡¶ø ‡¶è‡¶ú‡ßá‡¶®‡ßç‡¶ü ‡¶¨‡¶æ ‡¶Ö‡¶ü‡ßã‡¶Æ‡ßá‡¶∂‡¶® ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ‡ßá‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø‡•§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b6c1fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Function Call Output:\n",
      "\n",
      "```json\n",
      "{\n",
      " \"name\": \"get_current_weather\",\n",
      " \"arguments\": {\n",
      "  \"location\": \"Dhaka\",\n",
      "  \"unit\": \"celsius\"\n",
      " }\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ‡ßß. ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶≤‡ßç‡¶™‡¶®‡¶ø‡¶ï ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶≤‡¶ø‡¶∏‡ßç‡¶ü\n",
    "tools_schema = [\n",
    "    {\n",
    "        \"name\": \"get_current_weather\",\n",
    "        \"description\": \"Get the current weather in a given location\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"location\": {\"type\": \"string\", \"description\": \"The city and state, e.g. San Francisco, CA\"},\n",
    "                \"unit\": {\"type\": \"string\", \"enum\": [\"celsius\", \"fahrenheit\"]}\n",
    "            },\n",
    "            \"required\": [\"location\"]\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# ‡ß®. ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ ‡¶™‡ßç‡¶∞‡¶Æ‡ßç‡¶™‡¶ü (‡¶Æ‡¶°‡ßá‡¶≤‡¶ï‡ßá ‡¶∂‡ßá‡¶ñ‡¶æ‡¶®‡ßã ‡¶Ø‡ßá ‡¶∏‡ßá ‡¶è‡¶ï‡¶ü‡¶æ ‡¶ü‡ßÅ‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá)\n",
    "system_prompt = f\"\"\"You are a helpful assistant. You have access to the following tools:\n",
    "{json.dumps(tools_schema, indent=2)}\n",
    "\n",
    "If a user asks about weather, you MUST output a JSON object with 'name' and 'arguments'.\n",
    "Do not write normal text if a tool call is needed.\"\"\"\n",
    "\n",
    "# ‡ß©. ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶™‡ßç‡¶∞‡¶∂‡ßç‡¶®\n",
    "user_query = \"What's the weather like in Dhaka today?\"\n",
    "\n",
    "# ‡ß™. ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ï‡¶≤ ‡¶ï‡¶∞‡¶æ (‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶§‡ßã‡¶Æ‡¶æ‡¶∞ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ‡¶ü‡ßá‡¶ï‡ßç‡¶∏‡¶ü ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨)\n",
    "# ‡¶®‡ßã‡¶ü: ‡¶Ø‡¶¶‡¶ø ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø ‡¶•‡ßá‡¶ï‡ßá ‡¶Æ‡ßÅ‡¶õ‡ßá ‡¶ó‡¶ø‡ßü‡ßá ‡¶•‡¶æ‡¶ï‡ßá, ‡¶§‡¶¨‡ßá ‡¶Ü‡¶¨‡¶æ‡¶∞ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá‡•§\n",
    "# ‡¶ß‡¶∞‡ßá ‡¶®‡¶ø‡¶ö‡ßç‡¶õ‡¶ø 'model' ‡¶è‡¶¨‡¶Ç 'tokenizer' ‡¶≠‡ßá‡¶∞‡¶ø‡ßü‡ßá‡¶¨‡¶≤ ‡¶Ü‡¶ó‡ßá‡¶∞ ‡¶∏‡ßá‡¶≤ ‡¶•‡ßá‡¶ï‡ßá ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ‡¶Ü‡¶õ‡ßá‡•§\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": system_prompt + \"\\n\\nUser: \" + user_query}\n",
    "]\n",
    "\n",
    "input_ids = tokenizer.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "outputs = model.generate(input_ids, max_new_tokens=128, temperature=0.1) # Temperature ‡¶ï‡¶Æ ‡¶∞‡¶æ‡¶ñ‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡¶Ø‡¶æ‡¶§‡ßá JSON ‡¶≠‡ßÅ‡¶≤ ‡¶®‡¶æ ‡¶π‡ßü\n",
    "response = tokenizer.decode(outputs[0][len(input_ids[0]):], skip_special_tokens=True)\n",
    "\n",
    "print(\"Function Call Output:\\n\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c044652",
   "metadata": {},
   "source": [
    "### üì± ‡¶ß‡¶æ‡¶™ ‡ß™: ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶∏‡¶ø‡¶Æ‡ßÅ‡¶≤‡ßá‡¶∂‡¶® (8-bit Quantization)\n",
    "‡¶´‡ßã‡¶®‡ßá ‡¶¨‡¶æ ‡¶ï‡¶Æ ‡¶ï‡ßç‡¶∑‡¶Æ‡¶§‡¶æ‡¶∞ ‡¶°‡¶ø‡¶≠‡¶æ‡¶á‡¶∏‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ö‡¶æ‡¶≤‡¶æ‡¶®‡ßã‡¶∞ ‡¶™‡¶∞‡ßÄ‡¶ï‡ßç‡¶∑‡¶æ‡•§\n",
    "- **`load_in_8bit=True`**: ‡¶Æ‡¶°‡ßá‡¶≤‡ßá‡¶∞ ‡¶∏‡¶æ‡¶á‡¶ú ‡¶ï‡¶Æ‡¶ø‡ßü‡ßá ‡¶Ö‡¶∞‡ßç‡¶ß‡ßá‡¶ï ‡¶¨‡¶æ ‡¶§‡¶æ‡¶∞‡¶ì ‡¶ï‡¶Æ ‡¶ï‡¶∞‡ßá ‡¶´‡ßá‡¶≤‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§\n",
    "- **Gemma 3n**: ‡¶´‡¶æ‡¶á‡¶≤‡ßá‡¶∞ ‡¶Æ‡¶§‡ßá, ‡¶è‡¶á ‡¶Ü‡¶∞‡ßç‡¶ï‡¶ø‡¶ü‡ßá‡¶ï‡¶ö‡¶æ‡¶∞‡¶ü‡¶ø ‡¶Æ‡¶æ‡¶§‡ßç‡¶∞ ‡ß®-‡ß© ‡¶ú‡¶ø‡¶¨‡¶ø ‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø‡¶§‡ßá ‡¶∞‡¶æ‡¶® ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá, ‡¶Ø‡¶æ ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶∏‡¶ø‡¶Æ‡ßÅ‡¶≤‡ßá‡¶ü ‡¶ï‡¶∞‡ßá‡¶õ‡¶ø‡•§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fdf0354c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading google/gemma-2-2b-it in 8-bit mode (Mobile Simulation)...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49ceb673fe734deabfc298cb715bd80f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/838 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\SmartSearchSystem\\SmartSearchSystem\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\RUET\\.cache\\huggingface\\hub\\models--google--gemma-2-2b-it. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "056ee44a157e4386b128cc574b9b793b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/24.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "076f9281c16a4370a4fcdc2aebd5c29b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 2 files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bca71d07aaff47e3949394e8fd2a0ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/241M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10de7ea3461248969b2426abac2fc05d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/gemma-2-2b-it/resolve/299a8560bedf22ed1c72a8a11e7dce4a7f9f51f8/model-00001-of-00002.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37df2dc5fdf44663b6d261756a7a93b1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  14%|#3        | 682M/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://huggingface.co/google/gemma-2-2b-it/resolve/299a8560bedf22ed1c72a8a11e7dce4a7f9f51f8/model-00001-of-00002.safetensors: HTTPSConnectionPool(host='cas-bridge.xethub.hf.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1a3a7f24cf34a8ea429174164bc8296",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:  38%|###8      | 1.90G/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6f4563e4aea4eba8fb398354ea33019",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a324f8be3944047925ccd4f281dac20",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/187 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3783f30add51486bbbad3b4c1b3d4911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/47.0k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcb44cb3b2184f44b60b57139c3bac02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/4.24M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "994ec95347f6443dbe726a3ee2ce7673",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "625167aba81c49e1883ae1c411aa89c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/636 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mobile Response: Give me a 1-sentence motivation.\n",
      "\n",
      "**You are capable of achieving anything you set your mind to.** \n",
      "\n",
      "VRAM Used: 18.25 GB\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# ‡¶Æ‡ßã‡¶¨‡¶æ‡¶á‡¶≤ ‡¶∏‡¶ø‡¶Æ‡ßÅ‡¶≤‡ßá‡¶∂‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶Æ‡¶∞‡¶æ 2B ‡¶¨‡¶æ ‡¶õ‡ßã‡¶ü ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨\n",
    "mobile_model_id = \"google/gemma-2-2b-it\"\n",
    "\n",
    "print(f\"Loading {mobile_model_id} in 8-bit mode (Mobile Simulation)...\")\n",
    "\n",
    "# ‡ßÆ-‡¶¨‡¶ø‡¶ü ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶®\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    "    llm_int8_threshold=6.0\n",
    ")\n",
    "\n",
    "mobile_model = AutoModelForCausalLM.from_pretrained(\n",
    "    mobile_model_id,\n",
    "    quantization_config=quantization_config, # ‡¶è‡¶á ‡¶ï‡¶®‡¶´‡¶ø‡¶ó‡¶æ‡¶∞‡ßá‡¶∂‡¶® ‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø ‡¶ï‡¶Æ‡¶æ‡¶¨‡ßá\n",
    "    device_map=\"auto\",\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "mobile_tokenizer = AutoTokenizer.from_pretrained(mobile_model_id)\n",
    "\n",
    "# ‡¶ü‡ßá‡¶∏‡ßç‡¶ü\n",
    "prompt = \"Give me a 1-sentence motivation.\"\n",
    "inputs = mobile_tokenizer(prompt, return_tensors=\"pt\").to(mobile_model.device)\n",
    "outputs = mobile_model.generate(**inputs, max_new_tokens=50)\n",
    "print(\"Mobile Response:\", mobile_tokenizer.decode(outputs[0], skip_special_tokens=True))\n",
    "\n",
    "# ‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø ‡¶ö‡ßá‡¶ï\n",
    "print(f\"VRAM Used: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e7dc2bf",
   "metadata": {},
   "source": [
    "### üè≠ ‡¶ß‡¶æ‡¶™ ‡ß´: ‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡¶∂‡¶® ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏ (Real-world Deployment)\n",
    "‡¶è‡¶ü‡¶ø ‡¶è‡¶ï‡¶ü‡¶ø ‡¶™‡ßç‡¶∞‡¶´‡ßá‡¶∂‡¶®‡¶æ‡¶≤ ‡¶ï‡ßã‡¶°‡¶ø‡¶Ç ‡¶∏‡ßç‡¶ü‡ßç‡¶∞‡¶æ‡¶ï‡¶ö‡¶æ‡¶∞‡•§\n",
    "- **`ProductionGemmaService`**: ‡¶∏‡¶∞‡¶æ‡¶∏‡¶∞‡¶ø ‡¶ï‡ßã‡¶° ‡¶®‡¶æ ‡¶≤‡¶ø‡¶ñ‡ßá ‡¶è‡¶ï‡¶ü‡¶ø ‡¶ï‡ßç‡¶≤‡¶æ‡¶∏‡ßá‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶° ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§\n",
    "- **`health_check`**: ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∞‡ßç‡¶Ø‡¶æ‡¶∂ ‡¶ï‡¶∞‡ßá‡¶õ‡ßá ‡¶ï‡¶ø ‡¶®‡¶æ ‡¶¨‡¶æ ‡¶Æ‡ßá‡¶Æ‡ßã‡¶∞‡¶ø ‡¶´‡ßÅ‡¶≤ ‡¶ï‡¶ø ‡¶®‡¶æ ‡¶§‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶ø‡¶∏‡ßç‡¶ü‡ßá‡¶Æ‡•§\n",
    "- **`generate_async`**: ‡¶è‡¶ï‡¶∏‡¶æ‡¶•‡ßá ‡¶Ö‡¶®‡ßá‡¶ï ‡¶á‡¶â‡¶ú‡¶æ‡¶∞ ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡ßá‡¶≤ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ö‡¶∏‡¶ø‡¶ô‡ßç‡¶ï‡ßç‡¶∞‡ßã‡¶®‡¶æ‡¶∏ (Non-blocking) ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶®‡•§\n",
    "- **‡¶≤‡¶ó‡¶ø‡¶Ç (Logging)**: ‡¶ï‡¶§ ‡¶∏‡¶Æ‡ßü ‡¶≤‡¶æ‡¶ó‡¶≤ ‡¶¨‡¶æ ‡¶ï‡ßã‡¶®‡ßã ‡¶è‡¶∞‡¶∞ ‡¶π‡¶≤‡ßã ‡¶ï‡¶ø ‡¶®‡¶æ, ‡¶§‡¶æ ‡¶∞‡ßá‡¶ï‡¶∞‡ßç‡¶° ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶∏‡ßç‡¶•‡¶æ‡•§"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e9e733e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GemmaService:Loading model: google/gemma-2-9b-it...\n",
      "`torch_dtype` is deprecated! Use `dtype` instead!\n",
      "INFO:accelerate.utils.modeling:We will use 90% of the memory on device 0 for storing the model, and 10% for the buffer to avoid OOM. You can set `max_memory` in to a higher value to use more memory (at your own risk).\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b4a5161690348db8539914e8aadb017",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:accelerate.big_modeling:Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "INFO:GemmaService:Model loaded successfully!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System Health: {'status': 'healthy', 'vram_usage': '16.48 GB'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GemmaService:Generated in 95.04s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Response:\n",
      " ##  Microservices Architecture: A Deep Dive\n",
      "\n",
      "Microservices architecture is a software development approach where an application is built as a collection of small, independent services, each focused on a specific business capability.\n",
      "\n",
      "**Think of it like a well-oiled machine:**\n",
      "\n",
      "Instead of one giant, monolithic application, you have many smaller, interconnected parts, each performing a specific task.\n",
      "\n",
      "**Key Characteristics:**\n",
      "\n",
      "* **Independent Services:** Each service is self-contained, responsible for a single function and operates independently. They can be developed, deployed, and scaled independently.\n",
      "\n",
      "* **Loose Coupling:** Services communicate with each other through well-defined APIs, minimizing dependencies and allowing for flexibility and evolution.\n",
      "\n",
      "* **Domain-Driven Design:** Services are typically aligned with business capabilities or domains, reflecting the structure of the business itself.\n",
      "\n",
      "* **Decentralized Data Management:** Each service may have its own database, promoting data ownership and reducing data consistency concerns.\n",
      "* **Automation:** Continuous integration, continuous delivery (CI/CD) pipelines are essential for automating the development, testing, and deployment of microservices.\n",
      "\n",
      "**Benefits:**\n",
      "\n",
      "* **Increased Agility:** Develop and deploy features faster due to independent service development cycles.\n",
      "\n",
      "* **Improved Scalability:** Scale individual services based on demand, optimizing resource utilization.\n",
      "* **Enhanced Resilience:** Failures in one service are less likely to impact the entire application.\n",
      "* **Technology Diversity:** Teams can choose the best technology stack for each service, fostering innovation.\n",
      "* **Easier Maintenance:** Smaller codebases are easier to understand, debug, and maintain.\n",
      "\n",
      "**Challenges:**\n",
      "\n",
      "* **Increased Complexity:** Managing a distributed system with multiple services can be complex.\n",
      "* **Communication Overhead:** Inter-service communication can introduce latency and complexity.\n",
      "* **Data Consistency:** Maintaining data consistency across multiple databases can be challenging.\n",
      "* **Monitoring and Logging:** Monitoring and debugging distributed systems requires specialized tools and expertise.\n",
      "\n",
      "**When to Consider Microservices:**\n",
      "\n",
      "* Large, complex applications\n",
      "* Applications requiring high scalability and availability\n",
      "* Teams with diverse skillsets\n",
      "* Need for rapid development and deployment cycles\n",
      "\n",
      "**Conclusion:**\n",
      "\n",
      "Microservices architecture offers numerous advantages for modern software development, but it comes with its own set of challenges. Carefully evaluate your project requirements and technical capabilities before adopting this approach.\n",
      "\n",
      "Remember, microservices are not a magic bullet; they are a powerful tool that requires careful planning and execution.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import asyncio\n",
    "import time\n",
    "import logging\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "# ‡¶≤‡¶ó‡¶ø‡¶Ç ‡¶∏‡ßá‡¶ü‡¶Ü‡¶™ (‡¶Ø‡¶æ‡¶§‡ßá ‡¶ï‡¶®‡¶∏‡ßã‡¶≤‡ßá ‡¶∏‡¶¨ ‡¶á‡¶®‡¶´‡ßã ‡¶¶‡ßá‡¶ñ‡¶æ ‡¶Ø‡¶æ‡ßü)\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(\"GemmaService\")\n",
    "\n",
    "class ProductionGemmaService:\n",
    "    def __init__(self, model_name, device=\"auto\"):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.model = None\n",
    "        self.tokenizer = None\n",
    "        self._load_model()\n",
    "    \n",
    "    def _load_model(self):\n",
    "        \"\"\"‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≤‡ßã‡¶° ‡¶è‡¶¨‡¶Ç ‡¶è‡¶∞‡¶∞ ‡¶π‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶°‡¶≤‡¶ø‡¶Ç\"\"\"\n",
    "        try:\n",
    "            logger.info(f\"Loading model: {self.model_name}...\")\n",
    "            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                torch_dtype=torch.bfloat16, # RTX A4500 ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶∏‡ßá‡¶∞‡¶æ\n",
    "                device_map=self.device,\n",
    "            )\n",
    "            logger.info(\"Model loaded successfully!\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to load model: {str(e)}\")\n",
    "            raise e\n",
    "\n",
    "    async def generate_response(self, prompt, max_tokens=512):\n",
    "        \"\"\"‡¶Ö‡¶∏‡¶ø‡¶ô‡ßç‡¶ï‡ßç‡¶∞‡ßã‡¶®‡¶æ‡¶∏ ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶® (‡¶®‡¶®-‡¶¨‡ßç‡¶≤‡¶ï‡¶ø‡¶Ç)\"\"\"\n",
    "        start_time = time.time()\n",
    "        \n",
    "        try:\n",
    "            # ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏‡¶ø‡¶Ç\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\").to(self.model.device)\n",
    "            \n",
    "            # ‡¶ú‡ßá‡¶®‡¶æ‡¶∞‡ßá‡¶∂‡¶® (‡¶•‡ßç‡¶∞‡ßá‡¶°-‡¶∏‡ßá‡¶´ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø run_in_executor ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶≠‡¶æ‡¶≤‡ßã, ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶∏‡¶ø‡¶Æ‡ßç‡¶™‡¶≤ ‡¶∞‡¶æ‡¶ñ‡¶æ ‡¶π‡¶≤‡ßã)\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=max_tokens,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True\n",
    "                )\n",
    "            \n",
    "            response = self.tokenizer.decode(outputs[0][len(inputs.input_ids[0]):], skip_special_tokens=True)\n",
    "            \n",
    "            # ‡¶™‡¶æ‡¶∞‡¶´‡¶∞‡¶Æ‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶∏ ‡¶≤‡¶ó‡¶ø‡¶Ç\n",
    "            duration = time.time() - start_time\n",
    "            logger.info(f\"Generated in {duration:.2f}s\")\n",
    "            \n",
    "            return response.strip()\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Generation error: {str(e)}\")\n",
    "            return \"Error generating response.\"\n",
    "\n",
    "    def health_check(self):\n",
    "        \"\"\"‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶æ‡¶∞ ‡¶†‡¶ø‡¶ï ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø ‡¶®‡¶æ ‡¶ö‡ßá‡¶ï ‡¶ï‡¶∞‡¶æ\"\"\"\n",
    "        if self.model is not None:\n",
    "            vram = torch.cuda.memory_allocated() / 1024**3 if torch.cuda.is_available() else 0\n",
    "            return {\"status\": \"healthy\", \"vram_usage\": f\"{vram:.2f} GB\"}\n",
    "        return {\"status\": \"unhealthy\"}\n",
    "\n",
    "# --- ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ (Usage) ---\n",
    "\n",
    "# ‡ßß. ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏ ‡¶á‡¶®‡¶ø‡¶∂‡¶ø‡ßü‡¶æ‡¶≤‡¶æ‡¶á‡¶ú ‡¶ï‡¶∞‡¶æ\n",
    "# (‡¶®‡ßã‡¶ü: ‡¶ï‡¶æ‡¶∞‡ßç‡¶®‡ßá‡¶≤ ‡¶∞‡¶ø‡¶∏‡ßç‡¶ü‡¶æ‡¶∞‡ßç‡¶ü ‡¶®‡¶æ ‡¶ï‡¶∞‡¶≤‡ßá ‡¶è‡¶ñ‡¶æ‡¶®‡ßá OOM Error ‡¶ñ‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßã!)\n",
    "service = ProductionGemmaService(\"google/gemma-2-9b-it\") \n",
    "\n",
    "# ‡ß®. ‡¶π‡ßá‡¶≤‡¶• ‡¶ö‡ßá‡¶ï\n",
    "print(\"System Health:\", service.health_check())\n",
    "\n",
    "# ‡ß©. ‡¶Ö‡¶∏‡¶ø‡¶ô‡ßç‡¶ï‡ßç‡¶∞‡ßã‡¶®‡¶æ‡¶∏ ‡¶ï‡¶≤ (Jupyter ‡¶è await ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü)\n",
    "response = await service.generate_response(\"Explain microservices architecture properly.\")\n",
    "print(\"\\nAI Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "710bdbc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:GemmaService:Generated in 97.03s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AI Response:\n",
      " ## ‡¶Æ‡¶æ‡¶á‡¶ï‡ßç‡¶∞‡ßã‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏ ‡¶Ü‡¶∞‡ßç‡¶ï‡¶ø‡¶ü‡ßá‡¶ï‡¶ö‡¶æ‡¶∞: ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡ßÅ‡¶∏‡ßç‡¶™‡¶∑‡ßç‡¶ü ‡¶¨‡ßç‡¶Ø‡¶æ‡¶ñ‡ßç‡¶Ø‡¶æ\n",
      "\n",
      "‡¶Æ‡¶æ‡¶á‡¶ï‡ßç‡¶∞‡ßã‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏ ‡¶Ü‡¶∞‡ßç‡¶ï‡¶ø‡¶ü‡ßá‡¶ï‡¶ö‡¶æ‡¶∞ ‡¶π‡¶≤ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶∏‡¶´‡¶ü‡¶ì‡¶Ø‡¶º‡ßç‡¶Ø‡¶æ‡¶∞ ‡¶°‡¶ø‡¶ú‡¶æ‡¶á‡¶® ‡¶™‡¶¶‡ßç‡¶ß‡¶§‡¶ø ‡¶Ø‡¶æ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶¨‡¶°‡¶º ‡¶™‡ßç‡¶∞‡ßã‡¶ó‡ßç‡¶∞‡¶æ‡¶Æ‡¶ï‡ßá ‡¶õ‡ßã‡¶ü‡ßã, ‡¶∏‡ßç‡¶¨-‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£ ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏‡ßá ‡¶¨‡¶ø‡¶≠‡¶ï‡ßç‡¶§ ‡¶ï‡¶∞‡ßá‡•§ ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶Ö‡¶®‡ßç‡¶Ø‡¶æ‡¶®‡ßç‡¶Ø ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏‡ßá‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá API (Application Programming Interface) ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Ø‡ßã‡¶ó‡¶æ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡ßá‡•§ \n",
      "\n",
      "**‡¶™‡ßç‡¶∞‡¶ß‡¶æ‡¶® ‡¶¨‡ßà‡¶∂‡¶ø‡¶∑‡ßç‡¶ü‡ßç‡¶Ø:**\n",
      "\n",
      "* **‡¶õ‡ßã‡¶ü‡ßã ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶¨-‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£:** ‡¶™‡ßç‡¶∞‡¶§‡¶ø‡¶ü‡¶ø ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏ ‡¶è‡¶ï‡¶ü‡¶ø ‡¶®‡¶ø‡¶∞‡ßç‡¶¶‡¶ø‡¶∑‡ßç‡¶ü ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶¨‡¶Ø‡¶º‡¶Ç‡¶∏‡¶Æ‡ßç‡¶™‡ßÇ‡¶∞‡ßç‡¶£‡¶≠‡¶æ‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá‡•§\n",
      "* **‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶®‡ßç‡¶§‡¶∞‡¶Ø‡ßã‡¶ó‡ßç‡¶Ø:** ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã ‡¶∏‡ßç‡¶¨‡¶æ‡¶ß‡ßÄ‡¶®‡¶≠‡¶æ‡¶¨‡ßá ‡¶¨‡¶ø‡¶ï‡¶∂‡¶ø‡¶§, ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶® ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶® ‡¶è‡¶¨‡¶Ç ‡¶∏‡ßç‡¶•‡¶æ‡¶®‡¶æ‡¶®‡ßç‡¶§‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º‡•§\n",
      "* **‡¶™‡¶∞‡¶ø‡¶¨‡¶∞‡ßç‡¶§‡¶®‡¶∂‡ßÄ‡¶≤:** ‡¶®‡¶§‡ßÅ‡¶® ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶¨‡¶æ ‡¶™‡ßç‡¶∞‡¶∏‡ßç‡¶§‡¶æ‡¶¨‡¶ø‡¶§ ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶ï‡¶∞‡¶æ ‡¶∏‡¶π‡¶ú‡•§\n",
      "* **‡¶∏‡ßç‡¶ï‡ßá‡¶≤‡ßá‡¶¨‡ßá‡¶≤:** ‡¶™‡ßç‡¶∞‡¶Ø‡¶º‡ßã‡¶ú‡¶® ‡¶Ö‡¶®‡ßÅ‡¶Ø‡¶æ‡¶Ø‡¶º‡ßÄ ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶∏‡ßç‡¶ï‡ßá‡¶≤ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶Ø‡¶º, ‡¶Ø‡¶æ ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ‡¶¶‡ßá‡¶∞ ‡¶™‡ßç‡¶∞‡¶§‡ßç‡¶Ø‡¶æ‡¶∂‡¶æ‡¶∞ ‡¶∏‡¶æ‡¶•‡ßá ‡¶Æ‡¶ø‡¶≤ ‡¶∞‡¶æ‡¶ñ‡ßá‡•§\n",
      "\n",
      "**‡¶â‡¶¶‡¶æ‡¶π‡¶∞‡¶£:**\n",
      "\n",
      "‡¶ï‡¶≤‡ßç‡¶™‡¶®‡¶æ ‡¶ï‡¶∞‡ßÅ‡¶®, ‡¶è‡¶ï‡¶ü‡¶ø ‡¶á-‡¶ï‡¶Æ‡¶æ‡¶∞‡ßç‡¶∏ ‡¶ì‡¶Ø‡¶º‡ßá‡¶¨‡¶∏‡¶æ‡¶á‡¶ü‡•§ ‡¶è‡¶ü‡¶ø ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏‡ßá ‡¶¨‡¶ø‡¶≠‡¶ï‡ßç‡¶§ ‡¶ï‡¶∞‡¶æ ‡¶Ø‡ßá‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá, ‡¶Ø‡ßá‡¶Æ‡¶®:\n",
      "\n",
      "* **‡¶™‡ßç‡¶∞‡ßã‡¶°‡¶æ‡¶ï‡ßç‡¶ü ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏:** ‡¶™‡¶£‡ßç‡¶Ø ‡¶§‡¶•‡ßç‡¶Ø ‡¶π‡ßã‡¶∏‡ßç‡¶ü ‡¶ï‡¶∞‡ßá‡•§\n",
      "* **‡¶ï‡¶æ‡¶∞‡ßç‡¶ü ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏:** ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞‡¶ï‡¶æ‡¶∞‡ßÄ‡¶∞ ‡¶ï‡¶æ‡¶∞‡ßç‡¶ü ‡¶™‡¶∞‡¶ø‡¶ö‡¶æ‡¶≤‡¶®‡¶æ ‡¶ï‡¶∞‡ßá‡•§\n",
      "* **‡¶™‡ßá‡¶Æ‡ßá‡¶®‡ßç‡¶ü ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏:** ‡§≠‡•Å‡¶ó‡§§‡§æ‡§® ‡¶™‡ßç‡¶∞‡¶∏‡ßá‡¶∏ ‡¶ï‡¶∞‡ßá‡•§\n",
      "* **‡¶∂‡¶ø‡¶™‡¶ø‡¶Ç ‡¶∏‡¶æ‡¶∞‡ßç‡¶≠‡¶ø‡¶∏:** ‡¶∂‡¶ø‡¶™‡¶ø‡¶Ç ‡¶§\n"
     ]
    }
   ],
   "source": [
    "response = await service.generate_response(\"Explain microservices architecture properly in bangla.\")\n",
    "print(\"\\nAI Response:\\n\", response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633d2e63",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73804dc9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62fa3d2f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SmartSearchSystem",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
