# Gemma Model Family - Edge AI Implementation Guide

This repository contains the implementation of Google's **Gemma Model Family** (Gemma 3, Gemma 3n, and PaliGemma) optimized for local deployment on NVIDIA GPUs (specifically tested on RTX A4500 with 20GB VRAM).

## üöÄ Key Features Implemented
* **Basic Chat:** Text generation using Gemma-2-9B/Gemma-3-8B.
* **Multimodal Vision:** Image understanding using PaliGemma.
* **Function Calling:** Structured JSON output for API integrations.
* **Mobile Optimization:** 8-bit/4-bit quantization for edge devices (Gemma 3n simulation).
* **Production Service:** A robust Python class for real-world application deployment.

## üõ†Ô∏è Prerequisites
Ensure you have the following installed:
* Python 3.8+
* NVIDIA GPU with CUDA support
* Hugging Face Account (and User Access Token)

## üì¶ Installation

1.  **Install Dependencies:**
    ```bash
    pip install torch torchvision torchaudio --index-url [https://download.pytorch.org/whl/cu121](https://download.pytorch.org/whl/cu121)
    pip install transformers accelerate bitsandbytes pillow scipy
    ```

2.  **Hugging Face Login:**
    You must accept the license for Gemma models on Hugging Face and log in via terminal or code.
    ```python
    from huggingface_hub import login
    login(token="YOUR_HF_TOKEN")
    ```

## üíª Usage Examples

### 1. Basic Chat Inference
Running the standard instruction-tuned model.

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

model_id = "google/gemma-2-9b-it"
tokenizer = AutoTokenizer.from_pretrained(model_id)
model = AutoModelForCausalLM.from_pretrained(
    model_id,
    torch_dtype=torch.bfloat16,
    device_map="auto",
)

def chat(prompt):
    inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
    outputs = model.generate(**inputs, max_new_tokens=200)
    return tokenizer.decode(outputs[0], skip_special_tokens=True)

print(chat("Explain Quantum Computing."))
